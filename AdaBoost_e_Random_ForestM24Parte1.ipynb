{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJgE8QwT2gspD+HateyJAq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**M24  - AdaBoost e Random Forest**"
      ],
      "metadata": {
        "id": "aq39kvW1x3Bf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6b06ac7"
      },
      "source": [
        "| Caracter√≠stica                     | Random Forest                              | AdaBoost                                                        |\n",
        "| ---------------------------------- | ------------------------------------------ | --------------------------------------------------------------- |\n",
        "| **Tipo de Ensemble**               | *Bagging* (paralelo)                       | *Boosting* (sequencial)                                         |\n",
        "| **Como combina os modelos**        | M√©dia/vota√ß√£o das √°rvores independentes    | Modelos aprendem sequencialmente corrigindo os erros anteriores |\n",
        "| **Sensibilidade a ru√≠do/outliers** | Baixa sensibilidade (modelo robusto)       | Alta sensibilidade (boosting amplifica erros)                   |\n",
        "| **Tamanho das √°rvores**            | √Årvores completas (profundas)              | √Årvores rasas (stumps = profundidade 1)                         |\n",
        "| **Risco de overfitting**           | Baixo, gra√ßas ao bootstrap + aleatoriedade | Alto, principalmente se muitos estimadores forem usados         |\n",
        "| **Velocidade de treino**           | Mais lento (muitas √°rvores grandes)        | Mais r√°pido (√°rvores rasas e leves)                             |\n",
        "| **Interpretabilidade**             | M√©dia                                      | Baixa para m√©dia                                                |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ As 5 diferen√ßas entre Random Forest e AdaBoost\n",
        "\n",
        "üîπ Os 5 hiperpar√¢metros mais importantes do AdaBoost\n",
        "\n",
        "üîπ Quais s√£o os valores mais usados\n",
        "\n",
        "üîπ Qual √© o conjunto/base de dados mais usado\n",
        "\n",
        "üîπ Sugest√£o de par√¢metros para GridSearch\n",
        "\n",
        "üîπ C√≥digo pronto do AdaBoost (exemplo da scikit-learn)"
      ],
      "metadata": {
        "id": "afU8P6UfyIyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Cinco diferen√ßas entre Random Forest e AdaBoost**"
      ],
      "metadata": {
        "id": "0NUyTcHpybiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Caracter√≠stica                     | Random Forest                              | AdaBoost                                                        |\n",
        "| ---------------------------------- | ------------------------------------------ | --------------------------------------------------------------- |\n",
        "| **Tipo de Ensemble**               | *Bagging* (paralelo)                       | *Boosting* (sequencial)                                         |\n",
        "| **Como combina os modelos**        | M√©dia/vota√ß√£o das √°rvores independentes    | Modelos aprendem sequencialmente corrigindo os erros anteriores |\n",
        "| **Sensibilidade a ru√≠do/outliers** | Baixa sensibilidade (modelo robusto)       | Alta sensibilidade (boosting amplifica erros)                   |\n",
        "| **Tamanho das √°rvores**            | √Årvores completas (profundas)              | √Årvores rasas (stumps = profundidade 1)                         |\n",
        "| **Risco de overfitting**           | Baixo, gra√ßas ao bootstrap + aleatoriedade | Alto, principalmente se muitos estimadores forem usados         |\n",
        "| **Velocidade de treino**           | Mais lento (muitas √°rvores grandes)        | Mais r√°pido (√°rvores rasas e leves)                             |\n",
        "| **Interpretabilidade**             | M√©dia                                      | Baixa para m√©dia                                                |\n"
      ],
      "metadata": {
        "id": "akpccLq6yf_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Base de dados mais usada para exemplos de AdaBoost**\n",
        "\n",
        "A base mais usada pela Scikit-Learn √© o load_iris."
      ],
      "metadata": {
        "id": "oVK51DN6yoUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**classifica√ß√£o multiclasse**\n",
        "\n",
        "* tuning de hiperpar√¢metros\n",
        "\n",
        "* compara√ß√£o entre modelos\n",
        "\n",
        "* Outras muito usadas:\n",
        "\n",
        "* load_wine\n",
        "\n",
        "* load_breast_cancer (muito usada para modelos de boosting)\n",
        "\n",
        "* make_classification (gera√ß√£o de dados sint√©ticos)"
      ],
      "metadata": {
        "id": "LJS3k9It8WqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Cinco hiperpar√¢metros mais importantes do AdaBoost**"
      ],
      "metadata": {
        "id": "-A-In_4mzSAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. n_estimators**\n",
        "\n",
        "N√∫mero de modelos fracos gerados (quantas √°rvores rasas / stumps).\n",
        "\n",
        "**Valores mais usados:**\n",
        "\n",
        "50 (padr√£o)\n",
        "\n",
        "100\n",
        "\n",
        "200\n",
        "\n",
        "300\n",
        "\n",
        "**Influ√™ncia:**\n",
        "Quanto maior, mais complexo e melhor o ajuste ‚Äî mas pode causar overfitting.\n",
        "\n",
        "**2. learning_rate**\n",
        "\n",
        "Peso dado a cada novo modelo.\n",
        "\n",
        "**Valores mais usados:**\n",
        "\n",
        "1.0 (padr√£o)\n",
        "\n",
        "0.5\n",
        "\n",
        "0.1\n",
        "\n",
        "0.01 (para datasets mais complexos)\n",
        "\n",
        "** Regra pr√°tica:**\n",
        "Se aumentar os estimadores, geralmente reduzimos o learning_rate.\n",
        "\n",
        "**3. base_estimator**\n",
        "\n",
        "Qual √© o modelo fraco usado no boosting.\n",
        "\n",
        "**Mais usado:**\n",
        "\n",
        "**DecisionTreeClassifier(max_depth=1) ‚Üí decision stump (o mais cl√°ssico)**\n",
        "\n",
        " Outras op√ß√µes:\n",
        "\n",
        "**√Årvores mais profundas (max_depth=2, max_depth=3)**\n",
        "\n",
        "**Regress√£o log√≠stica**\n",
        "\n",
        "SVM linear (menos comum)\n",
        "\n",
        "**4. algorithm**\n",
        "\n",
        "Algoritmo interno do AdaBoost.\n",
        "\n",
        "Valores poss√≠veis:\n",
        "\n",
        "\"SAMME\" ‚Äì usado para multiclasse\n",
        "\n",
        "\"SAMME.R\" ‚Äì mais eficiente e mais usado hoje (padr√£o)\n",
        "\n",
        "**5. random_state**\n",
        "\n",
        "Garantir reprodutibilidade.\n",
        "\n",
        " Valores mais usados:\n",
        "\n",
        "42 (o favorito da comunidade)\n",
        "\n",
        "0\n",
        "\n",
        "7\n",
        "\n",
        "123"
      ],
      "metadata": {
        "id": "5LbgoUpgz32o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outros hiperpar√¢metros √∫teis**\n",
        "\n",
        "max_depth do base_estimator ‚Üí controla a complexidade da √°rvore\n",
        "\n",
        "min_samples_split ‚Üí evita overfitting\n",
        "\n",
        "min_samples_leaf ‚Üí for√ßa folhas maiores"
      ],
      "metadata": {
        "id": "6hVXB7wg0rVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Exemplos de combina√ß√µes usadas no GridSearch para AdaBoost**"
      ],
      "metadata": {
        "id": "mRGwDlcw0zfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = { \"n_estimators\": [50, 100, 200, 300], \"learning_rate\": [1.0, 0.5, 0.1, 0.01], \"algorithm\": [\"SAMME\", \"SAMME.R\"], \"base_estimator__max_depth\": [1, 2, 3] }"
      ],
      "metadata": {
        "id": "cJ-aJksN1QTT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. C√≥digo pronto (exemplo do AdaBoost com load_iris)**"
      ],
      "metadata": {
        "id": "ri64b2B31LFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregar base\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Separar treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Modelo base (stump)\n",
        "base = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# AdaBoost padr√£o\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=base,\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Treinar\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Avaliar\n",
        "pred = ada.predict(X_test)\n",
        "print(\"Acur√°cia:\", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K296wDrm1FKi",
        "outputId": "d70d0a0d-d97e-4a0e-a1e5-cad9b4737fe1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acur√°cia: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Modelo com AdaBoost + GridSearch**"
      ],
      "metadata": {
        "id": "BTgQarWL1mr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "\n",
        "# Suppress FutureWarning from sklearn\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# 1. CARREGAR BASE\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# 2. TREINO / TESTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. ESTIMADOR BASE\n",
        "base_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "\n",
        "# 4. MODELO ADABOOST\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=base_tree,   # para vers√µes novas\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 5. GRADE DE HIPERPAR√ÇMETROS (AJUSTADA)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"learning_rate\": [1.0, 0.5, 0.1],\n",
        "    \"algorithm\": [\"SAMME\"],            # <<< CORRE√á√ÉO AQUI !!!\n",
        "    \"estimator__max_depth\": [1, 2, 3]\n",
        "}\n",
        "\n",
        "# 6. GRIDSEARCH\n",
        "grid = GridSearchCV(\n",
        "    estimator=ada,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 7. RESULTADOS\n",
        "print(\"Melhores par√¢metros:\", grid.best_params_)\n",
        "print(\"Melhor score (cv):\", grid.best_score_)\n",
        "\n",
        "# 8. TESTE\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Acur√°cia no teste:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr1_VQHg59mM",
        "outputId": "204b8d56-cff3-458e-bce3-6c5469ae79a5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores par√¢metros: {'algorithm': 'SAMME', 'estimator__max_depth': 1, 'learning_rate': 0.5, 'n_estimators': 100}\n",
            "Melhor score (cv): 0.9523809523809523\n",
            "Acur√°cia no teste: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16c99e6c",
        "outputId": "79e2b128-0cb5-4e51-c902-ee446f191b8e"
      },
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress FutureWarning from sklearn\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"learning_rate\": [1.0, 0.5, 0.1],\n",
        "    \"algorithm\": [\"SAMME\"],\n",
        "    \"estimator__max_depth\": [1, 2, 3]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    AdaBoostClassifier(estimator=base_tree, random_state=42), # Changed `base` to `base_tree` to match above cell\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Melhores par√¢metros:\", grid.best_params_)\n",
        "print(\"Melhor score:\", grid.best_score_)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores par√¢metros: {'algorithm': 'SAMME', 'estimator__max_depth': 1, 'learning_rate': 0.5, 'n_estimators': 100}\n",
            "Melhor score: 0.9523809523809523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESUMO**\n",
        "\n",
        "Nesta pr√°tica, comparamos dois m√©todos de ensemble amplamente utilizados: Random Forest e AdaBoost. O Random Forest utiliza a t√©cnica de bagging, criando v√°rias √°rvores de decis√£o de forma paralela e combinando seus resultados por vota√ß√£o. J√° o AdaBoost utiliza boosting, criando modelos sequenciais que aprendem corrigindo os erros do modelo anterior.\n",
        "\n",
        "Exploramos tamb√©m a documenta√ß√£o oficial do AdaBoost no Scikit-Learn, implementando um exemplo com a base iris, uma das mais utilizadas em demonstra√ß√µes de modelos de classifica√ß√£o. Foi implementado um AdaBoost com √°rvore de decis√£o rasa (decision stump) como estimador base, e exploramos seus hiperpar√¢metros mais importantes, como n_estimators, learning_rate, algorithm, e a profundidade do estimador base.\n",
        "\n",
        "Em seguida, aplicamos o GridSearchCV para testar combina√ß√µes diferentes de hiperpar√¢metros. Durante essa etapa, identificamos uma mudan√ßa na vers√£o atual do Scikit-Learn: o par√¢metro algorithm='SAMME.R' n√£o √© mais aceito no GridSearch, exigindo a substitui√ß√£o por 'SAMME'. Ap√≥s o ajuste, o GridSearch executou corretamente, retornando os melhores hiperpar√¢metros e garantindo um aumento na performance do modelo."
      ],
      "metadata": {
        "id": "GGLp-PrF6sbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUS√ÉO**\n",
        "\n",
        "A pr√°tica demonstrou claramente que Random Forest e AdaBoost, embora ambos sejam ensembles, funcionam de formas distintas e adequadas para diferentes tipos de problema. O Random Forest se mostrou mais robusto a ru√≠dos e menos propenso a overfitting, enquanto o AdaBoost se destacou por sua capacidade de focar nos erros e melhorar sequencialmente o desempenho do modelo.\n",
        "\n",
        "A an√°lise refor√ßou a import√¢ncia dos hiperpar√¢metros no AdaBoost, especialmente n_estimators e learning_rate, que influenciam diretamente o equil√≠brio entre vi√©s e vari√¢ncia. O uso do GridSearchCV, ap√≥s corrigir o par√¢metro do algoritmo para 'SAMME', permitiu encontrar combina√ß√µes mais eficazes, aumentando a acur√°cia final do modelo.\n",
        "\n",
        "Assim, conclu√≠mos que o AdaBoost √© um m√©todo poderoso quando configurado corretamente, e que a compreens√£o dos hiperpar√¢metros, junto com ferramentas como GridSearch, √© essencial para extrair o m√°ximo desempenho desse algoritmo."
      ],
      "metadata": {
        "id": "nFeZH-to7HXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Item**                             | **Resumo**                                                                                                                |\n",
        "| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Compara√ß√£o**                       | Random Forest usa *bagging* (v√°rias √°rvores em paralelo). AdaBoost usa *boosting* (modelos sequenciais corrigindo erros). |\n",
        "| **Robustez**                         | Random Forest √© mais resistente a ru√≠dos; AdaBoost √© mais sens√≠vel.                                                       |\n",
        "| **Complexidade das √Årvores**         | Random Forest usa √°rvores profundas; AdaBoost usa √°rvores rasas (*stumps*).                                               |\n",
        "| **Base Usada**                       | Conjunto **iris** (padr√£o em exemplos do scikit-learn).                                                                   |\n",
        "| **Hiperpar√¢metros Mais Importantes** | `n_estimators`, `learning_rate`, `algorithm`, `max_depth`, `random_state`.                                                |\n",
        "| **Valores T√≠picos**                  | `n_estimators`: 50‚Äì200 ‚Ä¢ `learning_rate`: 1.0‚Äì0.1 ‚Ä¢ `algorithm`: \"SAMME\".                                                 |\n",
        "| **GridSearch**                       | Ajustado para usar `algorithm=\"SAMME\"` (SAMME.R n√£o funciona no GridSearch na vers√£o atual).                              |\n",
        "| **Resultado Final**                  | GridSearch encontrou melhores hiperpar√¢metros e aumentou a performance do AdaBoost.                                       |\n"
      ],
      "metadata": {
        "id": "HWFQYjz77Nkm"
      }
    }
  ]
}